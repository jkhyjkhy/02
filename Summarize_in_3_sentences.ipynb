{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnAhoDx8pFjHr2wr9f4y6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkhyjkhy/NLP_Project/blob/main/Summarize_in_3_sentences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0f72BNtE_VRk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "from _datetime import datetime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_soup(url): # soup 객체를 가져옴\n",
        "    res = requests.get(url)\n",
        "    if res.status_code == 200:\n",
        "        return bs(res.text, 'html.parser')\n",
        "    else:\n",
        "        print(f\"Super big fail! with {res.status_code}\")\n"
      ],
      "metadata": {
        "id": "bQ3ZX5jL_iqa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_head(soup):\n",
        "    head = soup.find('div', attrs={\"class\": \"news_headline\"})\n",
        "    date_info = head.find('div', attrs={\"class\": \"info\"})\n",
        "\n",
        "    print(\"기사 제목 : \" + head.select_one('.title').get_text())\n",
        "    print(\"기사 작성 시간 : \" + date_info.select_one('span').get_text())\n",
        "    # print(\"기자 이름 : \" + head.find('em', attrs ={\"class\" : \"media_end_head_journalist_name\"}).get_text()[0:3]) , 적용이 안되는 기자가 많아 예외 처리\n"
      ],
      "metadata": {
        "id": "8T4_uzRM_pJ9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(soup):\n",
        "    article = soup.find('div', attrs={\"id\": \"newsEndContents\"})\n",
        "\n",
        "    content = article.text.replace(\"\\n\",\"\")\n",
        "    content = re.split(\"[\\.?!]\\s+\", content) # 문장을 요소 단위로 분화, 문장 구분\n",
        "\n",
        "\n",
        "    return content"
      ],
      "metadata": {
        "id": "2OMYmOao_jxF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list(url): # 이 시각 많이 본 뉴스에서 리스트를 가져옴\n",
        "    print(f\"현재 시간은 {datetime.now()}입니다. 상위 10개의 기사를 조회합니다.\")\n",
        "    soup = get_soup(url)\n",
        "    # news_list = soup.find('ol', attrs={\"class\" : \"news_list\"})\n",
        "    news_list = soup.select('.news_list') # select를 써 본 기억이 없는것 같아 select로 구현\n",
        "    news_link = {} # 딕셔너리로 선언\n",
        "    for li in range(0, 10):\n",
        "        print(f\"{li+1}번 기사 : {news_list[0].findAll('a')[li].get_text()}\")\n",
        "        news_link[li] = news_list[0].findAll('a')[li].get('href')\n",
        "\n",
        "    print(\"================================\")\n",
        "\n",
        "    checking_number = int(input(\"조회하고 싶은 기사를 입력하세요 : \")) # 조회를 원하는 기사 본문의 url 저장\n",
        "\n",
        "    return \"https://sports.news.naver.com/\" + news_link[checking_number-1]"
      ],
      "metadata": {
        "id": "rxnouaA2_bA1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def textrank(x, df=0.85, max_iter=50): # df = Dumping Factor : 0.85라고 가정, max_iter = 최대 반복횟수, 제한 조건을 두고 적절한 값에 수렴할떄까지 반복해야 하나 임의적으로 50회라고 지정\n",
        "    assert 0 < df < 1\n",
        "\n",
        "    # initialize\n",
        "    A = normalize(x, axis=0, norm='l1')\n",
        "    R = np.ones(A.shape[0]).reshape(-1, 1)\n",
        "    bias = (1 - df) * np.ones(A.shape[0]).reshape(-1, 1)\n",
        "    # iteration\n",
        "    for _ in range(max_iter):\n",
        "        R = df * (A * R) + bias # @ 연산으로 한번에 계산해도 됨\n",
        "\n",
        "    return R"
      ],
      "metadata": {
        "id": "g9z-lMB4_Xh7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(content):\n",
        "    data = []\n",
        "    for text in content:\n",
        "        if (text == \"\" or len(text) == 0):\n",
        "            continue\n",
        "        elif text == \"All right reserved\": # 기사가 끝났으면 반복문 종료\n",
        "            break\n",
        "        temp_dict = dict()\n",
        "        temp_dict['sentence'] = text\n",
        "        temp_dict['token_list'] = text.split()  # 기초적인 띄어쓰기 단위로 나누기\n",
        "        # 한국어 전처리 분석기인 꼬꼬마 분석기 사용도 고려해봐야 할 것 같음\n",
        "        data.append(temp_dict)\n",
        "\n",
        "    data_frame = pd.DataFrame(data)\n",
        "    # print(data_frame) # 문장 리스트와 토큰화 된걸 보려면 주석 해제할 것\n",
        "    print(\"================================\")\n",
        "\n",
        "    # 여기서부터\n",
        "    # reference : https://hoonzi-text.tistory.com/68, 문서 요약 하기 (with textrank)\n",
        "    # reference2 : https://lovit.github.io/nlp/2019/04/30/textrank/, TextRank 를 이용한 키워드 추출과 핵심 문장 추출 (구현과 실험)\n",
        "\n",
        "    similarity_matrix = []\n",
        "    for i, row_i in data_frame.iterrows():\n",
        "        i_row_vec = []\n",
        "        for j, row_j in data_frame.iterrows():\n",
        "            if i == j:\n",
        "                i_row_vec.append(0.0)\n",
        "            else:\n",
        "                intersection = len(set(row_i['token_list']) & set(row_j['token_list'])) # 유사도 계산의 분자 부분\n",
        "                log_i = math.log(len(set(row_i['token_list'])))\n",
        "                log_j = math.log(len(set(row_j['token_list'])))\n",
        "                similarity = intersection / (log_i + log_j)\n",
        "                i_row_vec.append(similarity)\n",
        "\n",
        "        similarity_matrix.append(i_row_vec)\n",
        "\n",
        "        weightedGraph = np.array(similarity_matrix)\n",
        "\n",
        "    R = textrank(weightedGraph)\n",
        "    R = R.sum(axis=1)\n",
        "\n",
        "    indexs = R.argsort()[-3:] # 랭크값 상위 세 문장의 인덱스를 가져옴\n",
        "    for index in sorted(indexs): # 뉴스 구조의 순서를 유지하기 위해 등장한 순서대로 정렬함\n",
        "        print(data_frame['sentence'][index])\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "xs07tWoh_zdf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    url = get_list(\"https://sports.news.naver.com/wfootball/index\")\n",
        "    soup = get_soup(url)\n",
        "    get_head(soup)\n",
        "    print(\"================================\")\n",
        "    get_content(soup)\n",
        "    summarize_text(get_content(soup))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NZIhnQ3_2y7",
        "outputId": "20c87e68-1c6e-4541-ca16-5bc9645e1ea9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 시간은 2023-11-05 15:03:49.260696입니다. 상위 10개의 기사를 조회합니다.\n",
            "1번 기사 : 재계약 거부하고 이적 결심...토트넘 보강 계획 ‘청신호’ 잡혀\n",
            "2번 기사 : \"SON, 골 좀 그만 넣어\"…아스널 DF, 퀴즈쇼에서 '토트넘 캡틴' 손흥민 극찬\n",
            "3번 기사 : ‘이강인 멀티포지션 능력 덕분에 뎀벨레까지 살아났다!’ 엔리케 감독 함박웃음 지은 이유\n",
            "4번 기사 : '내 뒤에 부폰 있다!'…'요리스 삭제해 버린' 토트넘 신입 GK, 다 이유가 있었네! \"부폰이 항상 가치 있는 조언 해줘\"\n",
            "5번 기사 : \"이강인 처음봤을 때 놀랐다\" 700억 수비수 극찬…프랑스 매체들도 호평\n",
            "6번 기사 : 이대로 가면 51골! '美친 골감각' 케인, 레반도프스키 41골 기록 넘어 '역대 최다골 득점왕' 가능성 ↑\n",
            "7번 기사 : '포버지'가 밝힌 '매디슨X비카리오X판 더 펜 영입 성공 비결'...\"선수가 아닌 사람으로 본다\"\n",
            "8번 기사 : 손흥민 어떻게 잊나…\"포체티노 토트넘 복귀 원했다\"\n",
            "9번 기사 : 첼시 20살 DF의 도발 \"토트넘이 우릴 언제 이겼더라? 기억 안 나\"\n",
            "10번 기사 : 무대 옮겨도 잘한다...케인, '첫 10경기 15골' 분데스 역사상 최다 득점 신기록+첫 데어클라시커 해트트릭 주인공\n",
            "================================\n",
            "조회하고 싶은 기사를 입력하세요 : 2\n",
            "기사 제목 : \"SON, 골 좀 그만 넣어\"…아스널 DF, 퀴즈쇼에서 '토트넘 캡틴' 손흥민 극찬\n",
            "기사 작성 시간 : 기사입력 2023.11.05. 오후 10:01\n",
            "================================\n",
            "================================\n",
            "(엑스포츠뉴스 권동환 기자) 아스널 수비수 윌리엄 살리바가 라이벌 클럽 주장 손흥민(토트넘 홋스퍼) 사진을 단번에 맞추면서 칭찬을 아끼지 않았다.영국 '스카이스포츠'는 지난 4일(한국시간) 공식 유튜브 채널을 통해 \"윌리엄 살리바가 AI가 만든 프리미어리그 아기들을 추측했다\"라며 한 영상을 게시했다.2001년생 프랑스 센터백 살리바는 프리미어리그 명문 아스널 핵심 수비수 중 한 명이다\n",
            "\n",
            "당장 살리바는 지난 9월 2023/24시즌 프리미어리그 6라운드 홈에서 열린 토트넘과의 '북런던 더비'에서 선발로 나왔지만 손흥민을 막지 못해 2-2 무승부를 거뒀다.당시 아스널은 상대의 자책골로 앞서가고 있었지만 전반 42분 손흥민의 동점골로 리드를 잃어버렸다\n",
            "\n",
            "특히 손흥민은 이날 멀티골로 1993년 5월 존 헨드리 이후 약 30년 만에 프리미어리그 아스널 원정에서 2골을 터트린 토트넘 선수로 등극했다.손흥민을 상대로 힘겨운 경기를 펼친 살리바는 \"빠르고, 기술이 좋고, 굉장히 좋은 선수\"라며 \"골키퍼와 일대일 상황이 됐을 때 아주 깔끔하다\"라며 칭찬을 아끼지 않았다.이어 \"우리는 손흥민에게 골 넣는 것 좀 그만하라고 말해야 했다\"라고 말해 진행자의 웃음을 터트리게 했다\n",
            "\n"
          ]
        }
      ]
    }
  ]
}